{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: What are the three stages to build the hypotheses or model in machine learning?\n",
    "\n",
    "Answer 1: The three stages to build the hypotheses or model in maching learning are:\n",
    "a) Model building - It involves identifying the input paramters, type of model, etc.\n",
    "b) Model testing - It involves running the model so see the results and compare the output values with actual values\n",
    "c) Applying the model - It involves using the model for new dataset to predict the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2: What is the standard approach to supervised learning?\n",
    "\n",
    "Answer 2: The standard approach to supervised learning is to split the data into training set and test set.\n",
    "In supervised learning, a model is created by using labeled training data that consists of input data and an expected output. \n",
    "The expected output is compared with actual output and adjustments are done in the model based on that.\n",
    "When trained, you can apply this model to test set to verfify the output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3: What is Training set and Test set?\n",
    "\n",
    "Answer 3: \n",
    "Training Set:\n",
    "A training set is a dataset used to train the model. \n",
    "It is used to discover potentially predictive relationship and provide feature learnin for the model.\n",
    "\n",
    "Test Set:\n",
    "A test set is a dataset used to determine how well model performs at making predictions on that test set based on the learnings from the training set.\n",
    "It is used to test the accuracy of the model/hypotheses generated by the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4: What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "\n",
    "Answer 4:\n",
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm in order to improve robustness over a single model.\n",
    "Ensemble methods are meta-algorithms that combine several machine learning techniques into one predictive model in order to decrease variance (bagging), bias (boosting), or improve predictions (stacking).\n",
    "\n",
    "Bagging:\n",
    "Bagging stands for bootstrap aggregation. In this method, random samples of the training data set (sub sets of training data set) are created.\n",
    "Then, a classifier is built for each sample. Finally, results of these multiple classifiers are combined using average or majority voting. \n",
    "Bagging helps to reduce the variance error.\n",
    "\n",
    "Boosting:\n",
    "Boosting methods are used sequentially to reduce the bias of the combined model. The main principle of boosting is to fit a sequence of weak learners− models that are only slightly better than random guessing, such as small decision trees− to weighted versions of the data. \n",
    "More weight is given to examples that were misclassified by earlier rounds. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. \n",
    "Boosting has shown better predictive accuracy than bagging, but it also tends to over-fit the training data as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5: How can you avoid overfitting ?\n",
    "    \n",
    "Answer 5: \n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data.\n",
    "Overfitting can be avoided using below methods:\n",
    "1. Cross-validation: Use initial training set to generate multiple mini train-test splits and use these splits to tune the model. Example: k-fold cross validation\n",
    "2. Train with more data: Training with more data can help the algorithm to detect the signal better. \n",
    "3. Remove features: Use built-in feature-selection techniques for algorithms or remove the features which are not relevant.\n",
    "4. Ensembling: Use ensembling methods like bagging to reduce the chance of overfitting complex models\n",
    "5. Use simpler models/penalize complexity\n",
    "6. Early stopping: When a learning algorithm is being trained iteratively, it can be measured how well each iteration of the model performs.\n",
    "   Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data. \n",
    "   No more iterations should be performed after that threshold when there is no improvement in subsequent iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
